## Probabilistic Latent Semantic Analysis(pLSA)



### LSA

- 말뭉치 행령 A를 다음과 같이 분해

![LSA](https://i.imgur.com/YnroTPH.png)

- n개의 문서가 원래 단어 개수보다 훨씬 작은 q차원의 백처로 표현된 걸 확인할 수 있다. 
- 마찬가지로 m개 단어는 원래 문서 수보다 훨씬 작은 q차원 백터로 변환됐다. 
- q가 3이라면 전체 말뭉치가 3개의 토픽으로 분석됐다고도 말할 수 있을 것이다.



- 위 그램에서 행렬 L의 열벡터는 각각 해당 토픽에 대한 문서들의 분포 정보를 나타낸다. 
- R의 행벡터는 각각 해당 토픽에 대한 단어들의 분포 정도를 나타낸다.
- 중간에 대각행렬은 q개 토픽 각각이 전체 말뭉치 내에서 얼마나 중요한지 나타내는 가중치가 될 것이다.



### pLSA

- 단어와 문서 사이를 잇는, 우리 눈에 보이지 않는 잠재구조가 있다는 가정 하에 단어와 문서 출현 확률을 모델링한 확률모형이다.
- 아래 그림처럼 **Latent concepts**가 존재하고 이것이 문서와 단어를 연결한다고 가정한다.
- 문서들의 주제(토픽)이라고 생각하면 좋을 것 이다.

![pLSA](https://i.imgur.com/hiVmhJc.png)

- P(z|d)는 문서 하나가 주어졌을 때 특정 주제(토픽)가 나타날 확률을 의미한다.
- P(w|z)는 주제가 정해졌을 때 특정 단어가 나타날 확률을 가리킨다.

- 예컨대 위 그림 기준에서 네번째 문서는 trade라는 주제로만 구성되어 있다. 그런데 trade라는 주제는 economic, imports, trade 따위의 단어를 선호한다.

- 결과적으로 네번째 문서에는 economic, imports, trade 등의 단어가 출현할 가능성이 높다고 할 수 있다. 
- pLSA의 가정에 문제가 없다면 해당 문서에 실제 등장하는 단어의 출현 확률은 높아야 할 것이다.



![pLAS2](https://i.imgur.com/BXijX5M.png)

- (a) 우선 문서를 뽑는다. 그 다음 이 문서의 주제를 뽑는다. 마지막으로 해당 주제별로 단어를 뽑는다. 
- 사람이 글 쓸 때도 글을 쓰기로 마음을 먹고 나서 주제, 단어를 차례대로 결정하기 때문에 직관적으로 이해가 가능하다.
- 그런데 (b)는 좀 이해하기 까다롭다. 주제(z)를 뽑은 뒤 이 주제에 해당하는 문서와 단어를 뽑는 방식이다. pLSA는 바로 이 방식으로 모델이 구성되어 있다.



### LSA 와 pLSA

- LSA는 행렬 인수분해(matrix factorization), pLSA는 확률모형이다.

![LSApLSA](https://i.imgur.com/cHbHrOw.png)

- LSA 결과물인 행렬 Uk의 열벡터는 각각 해당 토픽에 대한 문서들의 분포 정보를 나타낸다. 이는 pLSA의 P(d|z)에 대응한다.

- 행렬 Vk의 행벡터는 각각 해당 토픽에 대한 단어들의 분포 정보를 나타낸다.이는 pLSA의 P(w|z)에 대응한다.

- Σk의 대각성분은 토픽 각각이 전체 말뭉치 내에서 얼마나 중요한지 나타내는 가중치가 된다. 이는 pLSA의 P(z)에 대응한다.

- pLSA의 결과물은 확률이기 때문에 각 요소값이 모두 0 이상, 전체 합이 1이 된다. 하지만 LSA는 이런 조건을 만족하지 않는다.